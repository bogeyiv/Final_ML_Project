---
title: "Final Project - STAT-627-002"
author: "Barbara Broussard, Adrian Bogart, and Rebecca Rogers"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
```

# Our Datasets
Our project uses data on intentional homicide rates, the functioning of the justice system and law enforcement, firearm trade, and sexual and violent crimes  from the United Nations Office on Drugs and Crimes (UNODC) database from <https://dataunodc.un.org/>. We selected the years 2010-2020 due to missing data.

```{r Data_Setup}
#Reading in clean UNODC homicide, justice, and sexual and violent crimes dataset
homicide <- read_excel("UNODC_summary2.xlsx")

# Selecting data between 2010 and 2020
homicide <- homicide %>%
  filter(year > 2009, year < 2021)

# Selecting variables of interest
homicide <- homicide[,c(1:3, 17:21, 36:38)]
#selected: subregion, year, female_homicide_rates, sc_kidnapping, sc_robbery, sc_serious_assault, sc_sexual_violence, sc_sv_rape, arrested_cautioned_suspected, prosecuted, convicted)

# Renaming the variables
colnames(homicide) <- c("subregion", "year", "femicide", "kidnapping", "robbery", "serious_assault", "sexual_violence", "rape", "arrested", "prosecuted", "convicted")


# Dropping missing data
homicide <- na.omit(homicide) 

# Homicide data set
head(homicide)
```

## Continuous Dependent Variable Dataset
The dataset for our machine learning models using a continuous measure of femicide (homicide_dummy), uses average femicide_rates by subregion as the dependent variable.

```{r homicide_dummy}
#Regression dataset with continuous dependent variable
##Creating a dummy variable for subregion
homicide_dummy <- homicide %>% 
  mutate(aus_nz = ifelse(subregion == "Australia and New Zealand", 1, 0)) %>% 
  mutate(cent_asia = ifelse(subregion == "Central Asia", 1, 0)) %>% 
  mutate(east_asia = ifelse(subregion == "Eastern Asia", 1, 0)) %>% 
  mutate(east_eu = ifelse(subregion == "Eastern Europe", 1, 0)) %>% 
  mutate(latin_am = ifelse(subregion == "Latin America and the Caribbean", 1, 0)) %>% 
  mutate(north_af = ifelse(subregion == "Northern Africa", 1, 0)) %>% 
  mutate(north_am = ifelse(subregion == "Northern America", 1, 0)) %>% 
  mutate(north_eu = ifelse(subregion == "Northern Europe", 1, 0)) %>% 
  mutate(south_east_asia = ifelse(subregion == "South-eastern Asia", 1, 0)) %>% 
  mutate(south_asia = ifelse(subregion =="Southern Asia", 1, 0)) %>% 
  mutate(south_eu = ifelse(subregion == "Southern Europe", 1, 0)) %>% 
  mutate(subS_africa = ifelse(subregion == "Sub-Saharan Africa", 1, 0)) %>% 
  mutate(west_asia = ifelse(subregion == "Western Asia", 1, 0)) %>% 
  mutate(west_eu = ifelse(subregion == "Western Europe", 1, 0))

#removing categorical variables
homicide_dummy <- homicide_dummy[, -1]

# Homicide_dummy dataset
head(homicide_dummy)
```

## Categorical Dependent Variable Dataset 
The dataset for our classification model (homicide_class) uses a categorical classification of femicide rates based on the quartiles of femicide rates. Since there are no international standards on the severity of femicide (what constitutes different levels of concerns for femicide in a country), we are separating countries into four groups to help prioritize subregions for policymakers.

```{r homicide_class}
# Create new variable with femicide categorical levels for for classification
summary(homicide$femicide)
homicide_class <- mutate(homicide,
                  femicide_class = case_when(
                    between(femicide,0,0.6139)  ~ 'Low',
                    between(femicide,0.6139,1.3291) ~ 'Medium',
                    between(femicide,1.3291,1.5109) ~ 'High',
                    between(femicide,1.5109,6.1714) ~ 'Critical'))
homicide_class$femicide_class <- as.factor(homicide_class$femicide_class)
homicide_class$femicide_class <- factor(homicide_class$femicide_class, levels = c("Critical", "High", "Medium", "Low"))
homicide_class<- homicide_class %>% 
  select(-femicide) %>% 
  na.omit()

#Renaming and factoring the subregions
homicide_class$subregion <- as.factor(homicide_class$subregion)
homicide_class$subregion <- dplyr::recode_factor(homicide_class$subregion, 
                                                 "Australia and New Zealand" = "AUZ_NZ", 
                                                                "Central Asia" = "C_Asia", 
                                                                "Eastern Asia" = "E_Asia", 
                                                                "Eastern Europe" = "E_Europe", 
                                                                "Latin America and the Caribbean" = "LA_Carib", 
                                                                "Melanesia" = "Melanesia", 
                                                                "Northern Africa" = "N_Africa", 
                                                                "Northern America" = "N_America", 
                                                                "Northern Europe" = "N_Europe", 
                                                                "South-eastern Asia" = "SE_Asia", 
                                                                "Southern Asia" = "S_Asia", 
                                                                "Southern Europe" = "S_Europe", 
                                                                "Sub-Saharan Africa" = "SubS_Africa", 
                                                                "Western Asia" = "W_Asia", 
                                                                "Western Europe" = "W_Europe")
#Homicide_class dataset
head(homicide_class)
```

## Library

```{r library, include = FALSE}
# Library
library(boot)
library(car)
library(class)
library(MASS)
library(gbm)
library(GGally)
library(glmnet)
library(leaps)
library(pls)
library(randomForest)
library(rpart)
library(rpart.plot)
```

# Descriptive Statistics
## Barplot: Regional Femicide Rates
Let's take a look at how the average femicide rates differ by subregions.

```{r}
#average femicide by subregion
regional_femicide <- homicide %>% 
  group_by(subregion) %>% 
  summarize(mean_femicide = mean(femicide)) %>% 
  arrange(desc(mean_femicide))

#barplot summarizing femicide rates by region
library(forcats)

#reordered by rate
ggplot(mapping = aes(x = fct_rev(fct_reorder(regional_femicide$subregion, 
                                             regional_femicide$mean_femicide)), 
                     y = regional_femicide$mean_femicide)) + 
  geom_col() +
  theme(axis.text.x = element_text(angle=90)) +
  ylab("Average Femicide Rate (per 100,000 persons)") +
  xlab("Subregion") +
  ggtitle("Average Femicide Rates by Subregion (2010-2020)") 
```

We can see that Latin America and Sub-saharan Africa have almost double the rates of femicide as the other regions. 

## Barplot: Annual Femicide Rates
Let's take a look at how the average femicide rates differ over time.

```{r}
#average femicide by year
annual_femicide <- homicide %>% 
  group_by(year) %>% 
  summarize(mean_femicide = mean(femicide))

annual_femicide$year <- as.character(annual_femicide$year)

#bar graph
ggplot(mapping = aes(x = annual_femicide$year, y = annual_femicide$mean_femicide)) + 
  geom_col() +
  ylab("Average Femicide Rates (per 100,000 persons)") +
  xlab("Year") +
  ggtitle("Average Global Femicide Rates (2010-2020)") 
```

We can see that there has been a general decline in femicide rates from 2010-2019. In 2020, there was a large spike in femicide rates, likely due to the COVID-19 pandemic where women were largely stuck at home. 

# Continous Models
## Full Linear Regression Model

```{r lm_model}
#Baseline full linear model
##Eliminating the Western Europe dummy variable [will be captured in the intercept]
lm_model <- lm(femicide~.,data=homicide_dummy[,-24])
summary(lm_model)

#calculating MSE
anova(lm_model)
```

In our baseline full linear model, we see that it does a fairly good job of accounting for the variability in femicide rates, explaining 87.87 percent of the variability. The most significant variables at the 0.05 level are regions (Central Asia, Eastern Asia, Eastern Europe, Latin America, Northern Europe, South Asia, Sub-Saharan Africa, and Western Europe), year, and several crimes (serious assault and rape). 

There is a negative association of femicide rates over time, which reflects the time-trend bar plot. The two significant crimes (rape and serious assault) have a positive correlation with femicide, which is to be expected because the murder of women is an escalatory  crime that is often associated with assaults and rapes. All of the regions with a significant association in predicting femicide rates have a positive association, with Sub-saharan Africa and Latin America having the highest coefficients (3.731 and 2.760 respectively). 

**Estimated Linear Model:**
E[femicide rates] = 116.6 - 0.05783(year) - 0.05316(kidnapping) + 0.001458(robbery) + 0.001680(serious_assault) - 0.004364(sexual_violence) + 0.01867(rape) - 0.0005684(arrested) + 0.0002752(prosecuted) - 0.00004050(convicted) + 0.05657(Australia and New Zealand) + 1.539(Central Asia) + 0.930(Eastern Asia) + 0.8717(Eastern Europe) + 2.760(Latin America) - 0.2890(Northern Africa) + 0.7222(Northern America) + 0.8157(Northern Europe) + 0.5914(South-eastern Asia) + 1.380(Southern Asia) + 0.4758(Southern Europe) + 3.731(Sub-Saharan Africa) + 0.5946(Western Africa) 

## Test for Multicollinearity: Variance Inflation Factor (VIF) score

```{r}
#linear model using the main data frame [eliminating the categorical variables]

#Uses VIF to check for collinearity 
collin=data.frame(Variance_Inflation_factor=vif(lm_model))
collin
```

When a variable has a VIF score above 5, it raises some concern for the validity of our model. Variables over a VIF score of 10 are often not considered useful for the model.

The highest VIF score among our variables is sexual_violence with a VIF of 27.983392. This is likely due to the inclusion of rape in the model which is reported as a sub-indicator of sexual_violence in the UNODC data. We plan to determine which of the two variables explains more of the variability in femicide rates through tuning methods like Lasso and Principal Component Regression.

Robbery also has a relatively high VIF of 10.360919. This could be due to its association with other crimes such as kidnapping and serious assaults. Again, our tuning models will help us to determine which variable is most significant to keep in our final model. 
  
When looking at the regions, both Latin America (which has a VIF of 9.844565) and North America (which has a VIF of 7.578729) have relatively concerning multi-collinearity scores. However, Latin America is highly significant, so it is likely that it will be included in the final mode. 
  
## Testing and Training Data
We split our data using 50% for the training data and 50% for the testing data.

```{r}
#set seed
set.seed(1234)

#testing and training data
n<-nrow(homicide_dummy)
train_rows <- sample(1:n, .5*n) 
homicide_dummy_train <- homicide_dummy[train_rows,]
homicide_dummy_test <- homicide_dummy[-train_rows,]
```

## Ridge Model

```{r ridge}
#Making training and testing matricies
x.train <- data.matrix(homicide_dummy_train[,-2])
x.test <- data.matrix(homicide_dummy_test[,-2])
y.train <- homicide_dummy$femicide[train_rows] 
y.test <- homicide_dummy$femicide[-train_rows] 

#Cross-validation ridge regression model
set.seed(1234)
ridge_cv <- cv.glmnet(x.train, y.train, type.measure = "mse", 
                        alpha = 0, family = "gaussian")
ridge_cv

#Plotting best lambdas for ridge model chosen through cross-validation
plot(ridge_cv)

#Best lambda value for the ridge model
best_lambda <- ridge_cv$lambda.1se
best_lambda

#Cross-validation ridge regression predictions
ridge_cv_predicted <- predict(ridge_cv, s = ridge_cv$lambda.1se, newx = x.test)

#Calculating MSE 
mean((y.test - ridge_cv_predicted)^2)

#Calculating r-squared
sst <- sum((y.test - mean(y.test))^2)
sse <- sum((ridge_cv_predicted - y.test)^2)
rsq <- 1 - sse/sst
rsq #0.6864701

#Final ridge model: lambda.1se
se_ridge_model <- glmnet(x.train, y.train, alpha = 0, lambda = best_lambda)
coef(se_ridge_model)
se_ridge_model$dev.ratio #0.9807353
```

The ridge model using dummy variables for the subregions has an MSE of 0.5209654 and an r-squared value of 0.6864701. It uses a lambda of 1.017153. 

**Final Ridge Model: [dummy]**
E[femicide_rate] = 82.3405746505 - 0.0403273291(year) - 0.0299350016(kidnapping) + 0.0033039429(robbery) + 0.0006559072(serious_assault) - 0.0009941848(sexual_violence) + 0.0108798846(rape) - 0.0003394732(arrested) - 0.0001360178(prosecuted) - 0.0000327672(convicted) - 0.3880073485(aus_nz) +  0.3594332362(central_asia) - 0.0307266088(east_asia) - 0.0469285414(east_eu) + 0.7125125640(latin_am) + 0.4727645959(north_af) - 0.1310964753(north_am) - 0.0512279956(north_eu) - 0.2752171457(south_east_asia) + 0.0802198567(south_asia) - 0.2681672822(south_eu) + 1.3661756282(SubS_africa) - 0.1088042332(west_asia) - 0.4719347567west_eu)

## Lasso Model

```{r lasso}
#Lasso regression cross-validation model
set.seed(1234)
lasso_cv <-cv.glmnet(x.train, y.train, type.measure = "mse", 
                         alpha = 1)
lasso_cv$lambda.1se

#Plot of lambdas chosen through cross-validation
plot(lasso_cv)

#Saving the best lambda chosen through cross-validation
se_lambda <- lasso_cv$lambda.1se
se_lambda

#Lasso regression predicted data
lasso_cv_predicted <- predict(lasso_cv, s = lasso_cv$lambda.1se, 
                                 newx = x.test)

#estimating the mse for the predicted values
mean((y.test - lasso_cv_predicted)^2)

#calculating r-squared
sst <- sum((y.test - mean(y.test))^2)
sse <- sum((lasso_cv_predicted - y.test)^2)
rsq <- 1 - sse/sst
rsq #0.6575109

#final lasso model: lambda.1se
se_lasso_model <- glmnet(x.train, y.train, alpha = 1, lambda = se_lambda)
coef(se_lasso_model)
```

The lasso model with the uses a lambda 1se of 0.1696714 and has an MSE of 0.5690844 and an R-squared value of 0.6575109. It has 4 variables.

**Final Lasso Model:**
E[femicide_rate] = 0.831095096 + 0.004403492(robbery) - 0.0004886450(sexual_violence) + 0.032739314(cent_asia) + 0.962101064(latin_am) - 0.016328790(north_af) + 2.248806284(subS_africa)

## Principal Component Regression (PCR) Model 

```{r}
set.seed(1234) # for the cross validation sampling

#setting up the linear model for PCR, Gives PC with cross validation score, as well as training variance explained in Response and predictors
pcrlm <- pcr(femicide ~ ., data = homicide_dummy_train, scale = TRUE,  center = TRUE, validation = "CV")
summary(pcrlm)

# Gives plot of all possible PC versus Predicted MSE
validationplot(pcrlm, val.type="MSEP")
```

Best number of Principal components for PCR is 14.
Justification: 14 PC and 16 PC were the smallest two in adjCV and CV respectively, but 14 PC only gained .0009 in CV in tradeoff to 16 PC which would increase adjCV by .0022. Additionally, 14 PC explained 95.15% of the variance in the response, and 16 PC only explained 97.54% of the variance in the response. The gain wan small in comparison to additional PC before, and did not seem enough to justify adding the additional 2 components to get to 16 PC. 

```{r PCR_PMSE}
set.seed(1234)

# Fitted LM with best number of PC, centered, so that the coefficients do not need a y intercept. 
pcrfit <- pcr (femicide~., data=homicide_dummy_train,  center =T, scale = TRUE , ncomp = 14)
summary(pcrfit)

# Gives Predicted MSE with test data & best number of PC
MSEP(pcrfit, newdata = homicide_dummy_test)$val[14]

# Gives Coefficients of the fitted LM for PCR
coef(pcrfit)
```

The PCR model using dummy variables for the subregions has an MSE of 0.2803102 and an r-squared value of .9515.

**Final PCR Model:**
E[femicide_rate] = -0.189452478(year) - 0.099930443(kidnapping) + 0.381007401(robbery) + 0.082085961(serious_assault) - 0.067825235(sexual_violence) + 0.137220512(rape) - 0.014421892(arrested) - 0.132707055(prosecuted) + 0.021855609(convicted) - 0.162462689(aus_nz) + 0.107814275(cent_asia) - 0.006409922(east_asia) - 0.021615036(east_eu) + 0.248733291(latin_am) - 0.167614535(north_af) - 0.053489856(north_am) - 0.022225007(north_eu) - 0.172082297(south_east_asia) + 0.054802996(south_asia) - 0.135706761(south_eu) + 0.597318696(subS_africa) - 0.012068911(west_asia) - 0.198137789(west_eu)

## Partial Least Squares Model (PLS)

```{r PLS}
set.seed(1234) # for the cross validation sampling

#setting up the linear model for PCR, Gives PC with cross validation score, as well as training variance explained in Response and predictors
plsrlm <- plsr(femicide ~ ., data = homicide_dummy_train, scale = TRUE, validation = "CV")
summary(plsrlm)

# Gives plot of all possible PC versus Predicted MSE
validationplot(plsrlm, val.type="MSEP")
```

Best number of Principal components for PLS is 3 in terms of CV and adjCV. 

```{r PLS_PMSE}
set.seed(1234)

# Fitted LM with best number of PC, centered, so that the coefficients do not need a y intercept. 
plsrfit <- plsr (femicide~., data=homicide_dummy_train, center = T, scale = TRUE , ncomp = 3)
summary(plsrfit)

# Gives Predicted MSE with test data & best number of PC
MSEP(plsrfit, newdata = homicide_dummy_test)$val[3]

# Gives Coefficients of the fitted LM for PCR
coef(plsrfit)
```

The PLSR model using dummy variables for the subregions has an MSE of 0.3210804 and an r-squared value of 0.9098. 

**Final PLSR Model:**
E[femicide_rate] = -0.2156446226(year) - 0.1083806577(kidnapping) + 0.3672539558(robbery) + 0.1024328046(serious_assault) - 0.0791101932(sexual_violence) + 0.1719873818(rape) - 0.0452366861(arrested) - 0.0627207842(prosecuted) + 0.0009834746(convicted) - 0.1782090838(aus_nz) + 0.1044214316(cent_asia) - 0.0099791345(east_asia) - 0.0191886712(east_eu) + 0.2625136964(latin_am) - 0.2004853334(north_af) - 0.0609475742(north_am) - 0.0203380471(north_eu) - 0.1387963653(south_east_asia) + 0.0384803398(south_asia) - 0.1251147835(south_eu) + 0.6001011583(subS_africa) - 0.0141131449(west_asia) - 0.1878332908(west_eu)

# Classification Models
## Trees (Classification)

```{r}
# With Subregion

## Create tree (The fitting uses K = 10 fold cross validation).
set.seed(1234)
trp <- rpart(femicide_class ~ ., data = homicide_class)
rpart.plot(trp) 
rpart.plot(trp, extra = 3) 
plotcp(trp)
trp

## Prune Tree
set.seed(1234)
trp$cptable
min_cp <-  trp$cptable[which.min(trp$cptable[,"xerror"]),"CP"]
min_cp
trpp <- prune(trp, cp = min_cp)
rpart.plot(trpp)
rpart.plot(trpp, extra = 3)
trpp

## Validation Set and Check Performance
set.seed(1234)
Z <-  sample(nrow(homicide_class), nrow(homicide_class)/2)
trpv <- rpart(femicide_class ~ ., data = homicide_class, subset = Z)
rpart.plot(trpv, faclen = 2, clip.facs = TRUE)
Yhat = predict(trpv, newdata = homicide_class[-Z,], type = "class")
summary(Yhat)
table(Yhat, homicide_class$femicide_class[-Z])
mean(Yhat != homicide_class$femicide_class[-Z], na.rm = TRUE)

## Random Trees
### Create new Z for random trees.
Z <-  sample(nrow(homicide_class), nrow(homicide_class)/2)

### Random Trees
set.seed(1234)
BAG <- randomForest(femicide_class ~ ., data = homicide_class, mtry = 5) # Compared all mtry combinations manually, 5 is the lowest misclassification rate.
BAG
importance(BAG)
varImpPlot(BAG)

### Boosting
set.seed(1234)
boosth <- gbm(femicide_class ~ ., data = homicide_class[Z,], n.trees = 10000)
summary(boosth)
```


```{r}
# Without Subregion

## Create tree (The fitting uses K = 10 fold cross validation).
set.seed(1234)
trp <- rpart(femicide_class ~ . - subregion, data = homicide_class)
rpart.plot(trp) 
rpart.plot(trp, extra = 3) 
plotcp(trp)
trp

## Prune Tree
set.seed(1234)
trp$cptable
min_cp <-  trp$cptable[which.min(trp$cptable[,"xerror"]),"CP"]
min_cp
trpp <- prune(trp, cp = min_cp)
rpart.plot(trpp)
rpart.plot(trpp, extra = 3)
trpp

## Validation Set and Check Performance
set.seed(1234)
Z <-  sample(nrow(homicide_class), nrow(homicide_class)/2)
trpv <- rpart(femicide_class ~ . - subregion, data = homicide_class, subset = Z)
rpart.plot(trpv, faclen = 2, clip.facs = TRUE)
Yhat = predict(trpv, newdata = homicide_class[-Z,], type = "class")
summary(Yhat)
table(Yhat, homicide_class$femicide_class[-Z])
mean(Yhat != homicide_class$femicide_class[-Z], na.rm = TRUE)

## Random Trees
### Create new Z for random trees.
Z <-  sample(nrow(homicide_class), nrow(homicide_class)/2)

### Random Trees
set.seed(1234)
BAG <- randomForest(femicide_class ~ . - subregion, data = homicide_class, mtry = 2) # Compared all mtry combinations manually, 2 is the lowest misclassification rate.
BAG
importance(BAG)
varImpPlot(BAG)

### Boosting
set.seed(1234)
boosth <- gbm(femicide_class ~ . - subregion, data = homicide_class[Z,], n.trees = 10000)
summary(boosth)
```

Method: Classification Tress

Diagrams: See Lines 278, 289, 325, 335.

Cross Validation Techniques: Unpruned tree uses K-fold cross validation (K = 10) by default. Additionally, see "Prune Tree". Intent is to find the best size of the tree and best predictors.

**Results:**

**Summary of Results (With Subregion):**
Unpruned and Untuned Tree: 
* Misclassification Rate = 53.49%
* Classification Rate = 46.51%.
* Variables: Subregion, Robbery

Pruned and Tuned Tree: 
* Misclassification Rate = 53.49%
* Classification Rate = 46.51%.
* Variables: Subregion, Robbery

Prediction Classification Rate: 
* Misclassification Rate = 24.62%
* Classification Rate = 75.38%

Random Trees/Boosting: 
* Misclassification Rate = 24.81%
* Classification Rate = 75.19%
* Variables: Subregion, Robbery, Arrested, Serious Assault, Kidnapping

**Summary of Results (Without Subregion):**
Unpruned and Untuned Tree: 
* Misclassification Rate = 53.49%
* Classification Rate = 46.51%.
* Variables: Robbery, Serious Assault, Arrested, Sexual Violence, Convicted

Pruned and Tuned Tree: 
* Misclassification Rate = 53.49%
* Classification Rate = 46.51%.
* Variables: Robbery, Serious Assault, Arrested, Sexual Violence, Convicted

Prediction Classification Rate: 
* Misclassification Rate = 41.54%
* Classification Rate = 58.46%

Random Trees/Boosting: 
* Misclassification Rate = 27.91%
* Classification Rate = 72.09%
* Variables: Robbery, Rape

## KNN
```{r}
# Create dataset with variables as numeric.
homicide_class_num <- homicide_class
homicide_class_num$subregion <- as.numeric(homicide_class_num$subregion)
homicide_class_num$femicide_class <- as.numeric(homicide_class_num$femicide_class)

## 1 is "Critical", 2 is "High", 3 is "Medium", 4 is "Low".

# With Subregion

## Set seed, training percentage, and create variables.
set.seed(1234)
homicide_class_num$femicide_class <- as.factor(homicide_class_num$femicide_class)
homicide_class_num$subregion <- as.factor(homicide_class_num$subregion)
training_pct <- .50
Z = sample(nrow(homicide_class_num), floor(training_pct*nrow(homicide_class_num)))
Xtrain = homicide_class_num[Z,] # Use c("X") to include predictors.
Ytrain = homicide_class_num$femicide_class[Z]
Xtest = homicide_class_num[-Z,] # Use c("X") to include predictors.
Yhat <- knn(Xtrain, Xtest, Ytrain, k = 1)
Ytest <- homicide_class_num$femicide_class[-Z]

## Compute confusion matrix.
conf_matrix <- table(Ytest, Yhat)
conf_matrix

## Tuning to determine optimal K and minimizes prediction error rate.

### Initialize data.
err_class <- rep(1:55)
tpr <- rep(1:55)
fpr <- rep(1:55)

### Run the loop.
for (k in 1:55){
  Yhat <- knn(Xtrain, Xtest, Ytrain, k = k) 
  err_class[k] <- mean(Yhat != Ytest) # The prediction is not correct.
  tpr[k] <- sum(Yhat == 1 & Ytest == 1) / sum(Ytest == 1) # TP/P.
  fpr[k] <- sum(Yhat == 1 & Ytest == 0) / sum(Ytest == 0) # FP/N.
}
ggplot(tibble(err_class, k = 1:55), aes(x = k, y = err_class)) +
  geom_line()

### Determine optimal K.
which.min(err_class)

### Determine probability of a mis-classification.
err_class[which.min(err_class)]

### Accuracy. 
1 - min(err_class)

## Predictions.
train <- homicide_class_num[c("subregion", "year", "kidnapping", "robbery", "serious_assault", "sexual_violence", "rape", "arrested", "prosecuted", "convicted")]
test <- tibble("subregion" == 1, "year" == 2016, "kidnapping" == .5, "robbery" = 47, "serious_assault" == 33, "sexual_violence" == 50, "rape" == 30, "arrested" == 280, "prosecuted" == 100, "convicted" == 23)
cl <- homicide_class_num$femicide_class
knn(train, test, cl, k = 1)
```

```{r}
# Without Subregion

## Remove Subregion
homicide_class_num <- homicide_class_num %>% 
  dplyr::select(-subregion)

## Set seed, training percentage, and create variables.
set.seed(1234)
homicide_class_num$femicide_class <- as.factor(homicide_class_num$femicide_class)
training_pct <- .50
Z = sample(nrow(homicide_class_num), floor(training_pct*nrow(homicide_class_num)))
Xtrain = homicide_class_num[Z,] # Use c("X") to include predictors.
Ytrain = homicide_class_num$femicide_class[Z]
Xtest = homicide_class_num[-Z,] # Use c("X") to include predictors.
Yhat <- knn(Xtrain, Xtest, Ytrain, k = 1)
Ytest <- homicide_class_num$femicide_class[-Z]

## Compute confusion matrix.
conf_matrix <- table(Ytest, Yhat)
conf_matrix

## Tuning to determine optimal K and minimizes prediction error rate.

### Initialize data.
err_class <- rep(1:55)
tpr <- rep(1:55)
fpr <- rep(1:55)

### Run the loop.
for (k in 1:55){
  Yhat <- knn(Xtrain, Xtest, Ytrain, k = k) 
  err_class[k] <- mean(Yhat != Ytest) # The prediction is not correct.
  tpr[k] <- sum(Yhat == 1 & Ytest == 1) / sum(Ytest == 1) # TP/P.
  fpr[k] <- sum(Yhat == 1 & Ytest == 0) / sum(Ytest == 0) # FP/N.
}
ggplot(tibble(err_class, k = 1:55), aes(x = k, y = err_class)) +
  geom_line()

### Determine optimal K.
which.min(err_class)

### Determine probability of a mis-classification.
err_class[which.min(err_class)]

### Accuracy. 
1 - min(err_class)

## Predictions.
train <- homicide_class_num[c("year", "kidnapping", "robbery", "serious_assault", "sexual_violence", "rape", "arrested", "prosecuted", "convicted")]
test <- tibble("year" == 2016, "kidnapping" == .5, "robbery" = 47, "serious_assault" == 33, "sexual_violence" == 50, "rape" == 30, "arrested" == 280, "prosecuted" == 100, "convicted" == 23)
cl <- homicide_class_num$femicide_class
knn(train, test, cl, k = 1)
```

Method: K Nearest Neighbors

Diagrams: See Lines 440 and 493.

Cross Validation Techniques: Conducted tuning to find the optimal k for KNN.

**Results:**

KNN (with subregion): 
* Misclassification Rate = 26.15%
* Classification Rate = 73.85%.

KNN (without subregion): 
* Misclassification Rate = 26.15%
* Classification Rate = 73.85%.

**Performance of Competing Classificaiton Methods:** 

Classification trees provide the best results compared to KNN.  Prediction rate of trees with subregion and both random trees (with and without subregion) are approximately 3% better than KNN.  Recommend using Classification Trees because it: 
1) Provides a visual for our desired audience
2) Has the highest classification rate (with and without using random trees)
3) The k for KNN is too little (K = 1). This suggests that $\hat{Y}$ will be unreliable because it is based on small set of data which could lead to overfitting. Classification trees present a better model because it is not overfitting.



